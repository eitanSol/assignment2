{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization which was recently proposed by [3].\n",
    "\n",
    "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
    "\n",
    "The authors of [3] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [3] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n",
    "\n",
    "It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n",
    "\n",
    "[3] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: Forward\n",
    "In the file `cs231n/layers.py`, implement the batch normalization forward pass in the function `batchnorm_forward`. Once you have done so, run the following to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [  3.44190317 -13.09089874  -5.00231188]\n",
      "  stds:  [ 30.44606386  28.28047715  33.36794365]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [ -1.11022302e-18  -1.68753900e-16  -7.63278329e-17]\n",
      "  std:  [ 0.99999999  0.99999999  1.        ]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 0.99999999  1.99999999  2.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print 'Before batch normalization:'\n",
    "print '  means: ', a.mean(axis=0)\n",
    "print '  stds: ', a.std(axis=0)\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print 'After batch normalization (gamma=1, beta=0)'\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print '  mean: ', a_norm.mean(axis=0)\n",
    "print '  std: ', a_norm.std(axis=0)\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print 'After batch normalization (nontrivial gamma, beta)'\n",
    "print '  means: ', a_norm.mean(axis=0)\n",
    "print '  stds: ', a_norm.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [ 0.06556101 -0.13477695 -0.08424621]\n",
      "  stds:  [ 1.02370645  1.0880946   0.97900982]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in xrange(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print 'After batch normalization (test-time):'\n",
    "print '  means: ', a_norm.mean(axis=0)\n",
    "print '  stds: ', a_norm.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: backward\n",
    "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
    "\n",
    "To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass.\n",
    "\n",
    "Once you have finished, run the following to numerically check your backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.69967622829e-07\n",
      "dgamma error:  1.67998029531e-12\n",
      "dbeta error:  5.52994035477e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dgamma error: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta error: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: alternative backward\n",
    "In class we talked about two different implementations for the sigmoid backward pass. One strategy is to write out a computation graph composed of simple operations and backprop through all intermediate values. Another strategy is to work out the derivatives on paper. For the sigmoid function, it turns out that you can derive a very simple formula for the backward pass by simplifying gradients on paper.\n",
    "\n",
    "Surprisingly, it turns out that you can also derive a simple expression for the batch normalization backward pass if you work out derivatives on paper and simplify. After doing so, implement the simplified batch normalization backward pass in the function `batchnorm_backward_alt` and compare the two implementations by running the following. Your two implementations should compute nearly identical results, but the alternative implementation should be a bit faster.\n",
    "\n",
    "NOTE: You can still complete the rest of the assignment if you don't figure this part out, so don't worry too much if you can't get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "dx difference:  9.04745861365e-13\n",
      "dgamma difference:  0.0\n",
      "dbeta difference:  0.0\n",
      "speedup: 0.72x\n"
     ]
    }
   ],
   "source": [
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print 'dx difference: ', rel_error(dx1, dx2)\n",
    "print 'dgamma difference: ', rel_error(dgamma1, dgamma2)\n",
    "print 'dbeta difference: ', rel_error(dbeta1, dbeta2)\n",
    "print 'speedup: %.2fx' % ((t2 - t1) / (t3 - t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Nets with Batch Normalization\n",
    "Now that you have a working implementation for batch normalization, go back to your `FullyConnectedNet` in the file `cs2312n/classifiers/fc_net.py`. Modify your implementation to add batch normalization.\n",
    "\n",
    "Concretely, when the flag `use_batchnorm` is `True` in the constructor, you should insert a batch normalization layer before each ReLU nonlinearity. The outputs from the last layer of the network should not be normalized. Once you are done, run the following to gradient-check your implementation.\n",
    "\n",
    "HINT: You might find it useful to define an additional helper layer similar to those in the file `cs231n/layer_utils.py`. If you decide to do so, do it in the file `cs231n/classifiers/fc_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.18007676906\n",
      "W1 relative error: 7.96e-06\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[[  7.06113612e-05  -3.28540313e-03  -8.75398681e-02   0.00000000e+00\n",
      "   -1.69290426e-04   2.76911782e-04  -1.09447265e-03   3.80579244e+00\n",
      "   -2.19016538e-04   0.00000000e+00   1.36628295e-02  -1.09357479e-04\n",
      "   -1.48830370e-04   1.52555384e-03  -3.29237119e-02   0.00000000e+00\n",
      "    1.42189188e-02   2.40856668e-05   0.00000000e+00  -7.24940077e-03]\n",
      " [ -9.52298240e-05   4.43084875e-03  -1.85452080e+00   0.00000000e+00\n",
      "    2.28313213e-04  -1.55555391e-04   1.47605719e-03  -2.13790956e+00\n",
      "   -4.63983421e-03   0.00000000e+00  -1.84263343e-02  -2.31672357e-03\n",
      "    2.00719596e-04  -8.56981019e-04  -6.97484383e-01   0.00000000e+00\n",
      "    3.01226076e-01   5.10249420e-04   0.00000000e+00   9.77688195e-03]\n",
      " [ -1.77665660e-05   8.26641355e-04   1.61898532e+00   0.00000000e+00\n",
      "    4.25952162e-05  -2.46081844e-04   2.75380740e-04  -3.38207410e+00\n",
      "    4.05054739e-03   0.00000000e+00  -3.43770801e-03   2.02248576e-03\n",
      "    3.74473341e-05  -1.35570539e-03   6.08899644e-01   0.00000000e+00\n",
      "   -2.62968499e-01  -4.45444703e-04   0.00000000e+00   1.82402373e-03]\n",
      " [  7.33892946e-06  -3.41470230e-04   4.99779185e-01   0.00000000e+00\n",
      "   -1.75952586e-05  -2.74321454e-05  -1.13754739e-04  -3.77019532e-01\n",
      "    1.25040029e-03   0.00000000e+00   1.42005354e-03   6.24339536e-04\n",
      "   -1.54688484e-05  -1.51128088e-04   1.87966762e-01   0.00000000e+00\n",
      "   -8.11780966e-02  -1.37508360e-04   0.00000000e+00  -7.53470530e-04]\n",
      " [ -5.55438584e-05   2.58433801e-03  -1.67375305e+00   0.00000000e+00\n",
      "    1.33165923e-04  -2.53246757e-05   8.60925464e-04  -3.48056422e-01\n",
      "   -4.18757098e-03   0.00000000e+00  -1.07473473e-02  -2.09090321e-03\n",
      "    1.17071908e-04  -1.39518375e-04  -6.29497752e-01   0.00000000e+00\n",
      "    2.71864315e-01   4.60513361e-04   0.00000000e+00   5.70246677e-03]\n",
      " [ -1.46365542e-04   6.81007559e-03  -4.61117820e+00   0.00000000e+00\n",
      "    3.50910190e-04  -4.45742998e-05   2.26865369e-03  -6.12617398e-01\n",
      "   -1.15367019e-02   0.00000000e+00  -2.83207063e-02  -5.76041037e-03\n",
      "    3.08499870e-04  -2.45567477e-04  -1.73425932e+00   0.00000000e+00\n",
      "    7.48985902e-01   1.26870805e-03   0.00000000e+00   1.50267613e-02]\n",
      " [  4.25932623e-05  -1.98177399e-03   2.28553024e+00   0.00000000e+00\n",
      "   -1.02116959e-04  -9.12689035e-05  -6.60192190e-04  -1.25437416e+00\n",
      "    5.71817744e-03   0.00000000e+00   8.24149606e-03   2.85515300e-03\n",
      "   -8.97753871e-05  -5.02815722e-04   8.59586700e-01   0.00000000e+00\n",
      "   -3.71234148e-01  -6.28836494e-04   0.00000000e+00  -4.37287984e-03]\n",
      " [  9.52633750e-05  -4.43239334e-03  -1.93751185e-01   0.00000000e+00\n",
      "   -2.28392527e-04   3.81943166e-04  -1.47657180e-03   5.24930134e+00\n",
      "   -4.84747220e-04   0.00000000e+00   1.84327577e-02  -2.42039944e-04\n",
      "   -2.00789696e-04   2.10418856e-03  -7.28697485e-02   0.00000000e+00\n",
      "    3.14706025e-02   5.33083799e-05   0.00000000e+00  -9.78029013e-03]\n",
      " [  1.85317073e-04  -8.62240941e-03   1.66272985e+00   0.00000000e+00\n",
      "   -4.44296133e-04   5.17692178e-04  -2.87239983e-03   7.11496931e+00\n",
      "    4.15999195e-03   0.00000000e+00   3.58575742e-02   2.07713273e-03\n",
      "   -3.90599464e-04   2.85205242e-03   6.25351934e-01   0.00000000e+00\n",
      "   -2.70073838e-01  -4.57480476e-04   0.00000000e+00  -1.90257633e-02]\n",
      " [  4.27999858e-06  -1.99138794e-04   2.14711406e-01   0.00000000e+00\n",
      "   -1.02612585e-05  -7.51976259e-06  -6.63393784e-05  -1.03348486e-01\n",
      "    5.37187672e-04   0.00000000e+00   8.28147972e-04   2.68224110e-04\n",
      "   -9.02107278e-06  -4.14272172e-05   8.07528797e-02   0.00000000e+00\n",
      "   -3.48751276e-02  -5.90753446e-05   0.00000000e+00  -4.39409442e-04]\n",
      " [  5.89732485e-05  -2.74390817e-03   1.78293578e+00   0.00000000e+00\n",
      "   -1.41388301e-04   2.62435851e-05  -9.14083520e-04   3.60685995e-01\n",
      "    4.46073551e-03   0.00000000e+00   1.14109430e-02   2.22729757e-03\n",
      "   -1.24300303e-04   1.44580947e-04   6.70561305e-01   0.00000000e+00\n",
      "   -2.89598657e-01  -4.90553598e-04   0.00000000e+00  -6.05456605e-03]\n",
      " [  1.63572045e-05  -7.61069119e-04   3.78342427e-01   0.00000000e+00\n",
      "   -3.92164301e-05   2.01135553e-05  -2.53536503e-04   2.76433825e-01\n",
      "    9.46577061e-04   0.00000000e+00   3.16501747e-03   4.72637085e-04\n",
      "   -3.44767548e-05   1.10808385e-04   1.42294445e-01   0.00000000e+00\n",
      "   -6.14533750e-02  -1.04096398e-04   0.00000000e+00  -1.67933600e-03]\n",
      " [ -7.02190972e-05   3.26714302e-03  -2.80252651e+00   0.00000000e+00\n",
      "    1.68349557e-04   4.38239889e-05   1.08838969e-03   6.02304875e-01\n",
      "   -7.01165024e-03   0.00000000e+00  -1.35868917e-02  -3.50099907e-03\n",
      "    1.48003232e-04   2.41433828e-04  -1.05402848e+00   0.00000000e+00\n",
      "    4.55208962e-01   7.71081421e-04   0.00000000e+00   7.20910873e-03]\n",
      " [  2.45325538e-05  -1.14144774e-03   2.50458625e+00   0.00000000e+00\n",
      "   -5.88163740e-05  -1.83820736e-04  -3.80252652e-04  -2.52637991e+00\n",
      "    6.26623373e-03   0.00000000e+00   4.74687598e-03   3.12880395e-03\n",
      "   -5.17081045e-05  -1.01269937e-03   9.41973462e-01   0.00000000e+00\n",
      "   -4.06815028e-01  -6.89107016e-04   0.00000000e+00  -2.51865915e-03]\n",
      " [ -1.15259358e-04   5.36277933e-03   1.87235698e+00   0.00000000e+00\n",
      "    2.76333623e-04  -6.43049658e-04   1.78651309e-03  -8.83780539e+00\n",
      "    4.68445860e-03   0.00000000e+00  -2.23019061e-02   2.33900508e-03\n",
      "    2.42936560e-04  -3.54266838e-03   7.04192557e-01   0.00000000e+00\n",
      "   -3.04123173e-01  -5.15156828e-04   0.00000000e+00   1.18332316e-02]]\n",
      "W2 relative error: 1.05e-06\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[[  1.14510202e-02  -1.10418601e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84405296e+00  -1.86582832e-02  -8.42492742e-06  -5.80206327e-04\n",
      "    1.92269840e-03   0.00000000e+00  -3.30167428e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.99428967e-05   0.00000000e+00\n",
      "   -5.53029000e-03   0.00000000e+00   0.00000000e+00  -6.93295627e-01\n",
      "   -5.21406827e-03   0.00000000e+00   2.41299958e-04   0.00000000e+00\n",
      "    1.34586374e-01   0.00000000e+00   5.12767340e-04   0.00000000e+00\n",
      "    1.00341202e-04   1.12363059e-03]\n",
      " [  1.14348911e-02  -1.10263070e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84145556e+00  -1.86320025e-02  -8.41307024e-06  -5.79389070e-04\n",
      "    1.91999023e-03   0.00000000e+00  -3.29702379e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.98302979e-05   0.00000000e+00\n",
      "   -5.52250041e-03   0.00000000e+00   0.00000000e+00  -6.92319100e-01\n",
      "   -5.20672410e-03   0.00000000e+00   2.40960074e-04   0.00000000e+00\n",
      "    1.34396805e-01   0.00000000e+00   5.12045073e-04   0.00000000e+00\n",
      "    1.00199848e-04   1.12204794e-03]\n",
      " [ -9.43373406e-03   9.09665454e-04   0.00000000e+00  -4.71398152e-03\n",
      "   -1.51919271e+00   1.53713187e-02   0.00000000e+00   4.77993423e-04\n",
      "   -1.58398348e-03  -2.78993735e-03   2.72003080e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.38231168e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.05693665e-04   0.00000000e+00   5.71160110e-01\n",
      "    4.29552429e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.10876752e-01   0.00000000e+00   0.00000000e+00   4.48829551e-04\n",
      "   -8.26644309e-05  -9.25684573e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.14495135e-02  -1.10404070e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84381032e+00  -1.86558282e-02  -8.42381720e-06  -5.80129966e-04\n",
      "    1.92244543e-03   0.00000000e+00  -3.30123985e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.99323940e-05   0.00000000e+00\n",
      "   -5.52956232e-03   0.00000000e+00   0.00000000e+00  -6.93204405e-01\n",
      "   -5.21338219e-03   0.00000000e+00   2.41268205e-04   0.00000000e+00\n",
      "    1.34568665e-01   0.00000000e+00   5.12699860e-04   0.00000000e+00\n",
      "    1.00327968e-04   1.12348273e-03]\n",
      " [  1.14485745e-02  -1.10395015e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84365911e+00  -1.86542983e-02  -8.42312886e-06  -5.80082404e-04\n",
      "    1.92228775e-03   0.00000000e+00  -3.30096912e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.99258437e-05   0.00000000e+00\n",
      "   -5.52910886e-03   0.00000000e+00   0.00000000e+00  -6.93147556e-01\n",
      "   -5.21295465e-03   0.00000000e+00   2.41248421e-04   0.00000000e+00\n",
      "    1.34557629e-01   0.00000000e+00   5.12657783e-04   0.00000000e+00\n",
      "    1.00319730e-04   1.12339058e-03]\n",
      " [  1.14467019e-02  -1.10376959e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84335754e+00  -1.86512469e-02  -8.42172998e-06  -5.79987525e-04\n",
      "    1.92197334e-03   0.00000000e+00  -3.30042918e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.99127875e-05   0.00000000e+00\n",
      "   -5.52820447e-03   0.00000000e+00   0.00000000e+00  -6.93034177e-01\n",
      "   -5.21210197e-03   0.00000000e+00   2.41208942e-04   0.00000000e+00\n",
      "    1.34535620e-01   0.00000000e+00   5.12573961e-04   0.00000000e+00\n",
      "    1.00303343e-04   1.12320686e-03]\n",
      " [  8.69049526e-03  -8.37997272e-04   0.00000000e+00   0.00000000e+00\n",
      "    1.39950280e+00  -1.41602859e-02  -6.39390763e-06  -4.40334613e-04\n",
      "    1.45918899e-03   0.00000000e+00  -2.50573285e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -6.06708905e-05   0.00000000e+00\n",
      "   -4.19708996e-03   0.00000000e+00   0.00000000e+00  -5.26161119e-01\n",
      "   -3.95710047e-03   0.00000000e+00   1.83129223e-04   0.00000000e+00\n",
      "    1.02141299e-01   0.00000000e+00   3.89153310e-04   0.00000000e+00\n",
      "    7.61516850e-05   8.52754356e-04]\n",
      " [ -1.14376392e-02   1.10289571e-03   0.00000000e+00  -5.71532097e-03\n",
      "   -1.84189811e+00   1.86364803e-02   0.00000000e+00   5.79528314e-04\n",
      "   -1.92045166e-03  -3.38257311e-03   3.29781615e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.88836000e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.13112627e-04   0.00000000e+00   6.92485484e-01\n",
      "    5.20797541e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.34429104e-01   0.00000000e+00   0.00000000e+00   5.44169509e-04\n",
      "   -1.00223918e-04  -1.12231757e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [ -1.13537329e-02   1.09480487e-03   0.00000000e+00  -5.67339347e-03\n",
      "   -1.82838599e+00   1.84997634e-02   0.00000000e+00   5.75276893e-04\n",
      "   -1.90636327e-03  -3.35775860e-03   3.27362349e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.86717072e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.08614870e-04   0.00000000e+00   6.87405421e-01\n",
      "    5.16976981e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.33442934e-01   0.00000000e+00   0.00000000e+00   5.40177503e-04\n",
      "   -9.94886840e-05  -1.11408429e-03]\n",
      " [ -1.14422447e-02   1.10333980e-03   0.00000000e+00  -5.71762226e-03\n",
      "   -1.84263976e+00   1.86439844e-02   0.00000000e+00   5.79761683e-04\n",
      "   -1.92122493e-03  -3.38393511e-03   3.29914404e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.88952284e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.13359497e-04   0.00000000e+00   6.92764318e-01\n",
      "    5.21007242e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.34483233e-01   0.00000000e+00   0.00000000e+00   5.44388623e-04\n",
      "   -1.00264286e-04  -1.12276948e-03]\n",
      " [ -1.14504634e-02   1.10413230e-03   0.00000000e+00  -5.72172911e-03\n",
      "   -1.84396329e+00   1.86573760e-02   0.00000000e+00   5.80178106e-04\n",
      "   -1.92260492e-03  -3.38636572e-03   3.30151374e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.89159829e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.13800077e-04   0.00000000e+00   6.93261916e-01\n",
      "    5.21381471e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.34579830e-01   0.00000000e+00   0.00000000e+00   5.44779644e-04\n",
      "   -1.00336295e-04  -1.12357594e-03]\n",
      " [  1.14460758e-02  -1.10370924e-03   0.00000000e+00   0.00000000e+00\n",
      "    1.84325672e+00  -1.86502269e-02  -8.42128589e-06  -5.79955817e-04\n",
      "    1.92186822e-03   0.00000000e+00  -3.30024867e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -7.99083910e-05   0.00000000e+00\n",
      "   -5.52790209e-03   0.00000000e+00   0.00000000e+00  -6.92996272e-01\n",
      "   -5.21181689e-03   0.00000000e+00   2.41195774e-04   0.00000000e+00\n",
      "    1.34528261e-01   0.00000000e+00   5.12545917e-04   0.00000000e+00\n",
      "    1.00297859e-04   1.12314544e-03]\n",
      " [ -1.08575863e-02   1.04696305e-03   0.00000000e+00  -5.42547200e-03\n",
      "   -1.74848741e+00   1.76913426e-02   0.00000000e+00   5.50137913e-04\n",
      "   -1.82305717e-03  -3.21102802e-03   3.13056975e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.74187872e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.82019033e-04   0.00000000e+00   6.57366486e-01\n",
      "    4.94385617e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.27611610e-01   0.00000000e+00   0.00000000e+00   5.16572296e-04\n",
      "   -9.51411394e-05  -1.06539997e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [ -1.10261502e-02   1.06321714e-03   0.00000000e+00  -5.50970234e-03\n",
      "   -1.77563264e+00   1.79660003e-02   0.00000000e+00   5.58678792e-04\n",
      "   -1.85136009e-03  -3.26087917e-03   3.17917169e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.78444645e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.91054850e-04   0.00000000e+00   6.67572095e-01\n",
      "    5.02060944e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.29592779e-01   0.00000000e+00   0.00000000e+00   5.24592059e-04\n",
      "   -9.66182023e-05  -1.08194027e-03]\n",
      " [ -1.14442353e-02   1.10353178e-03   0.00000000e+00  -5.71861700e-03\n",
      "   -1.84296034e+00   1.86472280e-02   0.00000000e+00   5.79862536e-04\n",
      "   -1.92155918e-03  -3.38452382e-03   3.29971801e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.89002555e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.13466211e-04   0.00000000e+00   6.92884843e-01\n",
      "    5.21097887e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.34506630e-01   0.00000000e+00   0.00000000e+00   5.44483325e-04\n",
      "   -1.00281738e-04  -1.12296483e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [ -1.14315402e-02   1.10230762e-03   0.00000000e+00  -5.71227332e-03\n",
      "   -1.84091594e+00   1.86265426e-02   0.00000000e+00   5.79219295e-04\n",
      "   -1.91942757e-03  -3.38076938e-03   3.29605763e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.88681967e-04   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -6.12785689e-04   0.00000000e+00   6.92116222e-01\n",
      "    5.20519832e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -1.34357421e-01   0.00000000e+00   0.00000000e+00   5.43879319e-04\n",
      "   -1.00170472e-04  -1.12171912e-03]]\n",
      "W3 relative error: 1.22e-09\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[[ 0.05055195 -0.43029637  0.04274106  0.05925049  0.04767784  0.04331711\n",
      "   0.04426666  0.0446766   0.0486768   0.04913786]\n",
      " [ 0.05307269  0.04736209  0.05241347  0.05731072  0.03404464  0.05668757\n",
      "   0.04705002 -0.44473864  0.04629512  0.05050231]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.05120349 -0.43584224  0.04329192  0.06001414  0.04829234  0.0438754\n",
      "   0.04483719  0.04525241  0.04930417  0.04977118]\n",
      " [ 0.02227184 -0.1895771   0.01883057  0.02610418  0.02100559  0.01908436\n",
      "   0.01950271  0.01968332  0.0214457   0.02164883]\n",
      " [ 0.05198332  0.04638994  0.05133763  0.05613437  0.03334584  0.055524\n",
      "   0.04608427 -0.43560994  0.04534487  0.04946571]\n",
      " [ 0.05326297  0.04753189  0.05260138  0.05751619  0.0341667   0.0568908\n",
      "   0.0472187  -0.44633311  0.0464611   0.05068338]\n",
      " [ 0.05320322  0.04747858  0.05254238  0.05745168  0.03412837  0.05682699\n",
      "   0.04716574 -0.44583247  0.04640898  0.05062652]\n",
      " [ 0.05161451 -0.43934081  0.04363944  0.06049588  0.04867999  0.04422759\n",
      "   0.04519711  0.04561566  0.04969994  0.0501707 ]\n",
      " [ 0.05152219 -0.43855501  0.04356138  0.06038768  0.04859292  0.04414849\n",
      "   0.04511627  0.04553407  0.04961105  0.05008096]\n",
      " [ 0.0125395   0.01119026  0.01238375  0.01354083  0.00804374  0.01339359\n",
      "   0.01111653 -0.10507856  0.01093817  0.0119322 ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.05166591 -0.43977838  0.0436829   0.06055613  0.04872847  0.04427164\n",
      "   0.04524212  0.04566109  0.04974944  0.05022067]\n",
      " [ 0.05327374  0.0475415   0.05261202  0.05752782  0.0341736   0.0569023\n",
      "   0.04722825 -0.44642335  0.04647049  0.05069362]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.05305012  0.04734194  0.05239118  0.05728635  0.03403016  0.05666345\n",
      "   0.04703001 -0.44454948  0.04627543  0.05048083]\n",
      " [ 0.05164405 -0.43959226  0.04366441  0.0605305   0.04870785  0.04425291\n",
      "   0.04522298  0.04564177  0.04972839  0.05019941]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.03021909 -0.25722382  0.02554987  0.03541893  0.028501    0.02589423\n",
      "   0.02646186  0.02670691  0.02909816  0.02937378]\n",
      " [ 0.0512105  -0.43590195  0.04329786  0.06002236  0.04829895  0.04388141\n",
      "   0.04484334  0.04525861  0.04931092  0.04977799]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.0532713   0.04753932  0.05260961  0.05752519  0.03417204  0.0568997\n",
      "   0.04722609 -0.44640291  0.04646836  0.0506913 ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.04829267 -0.41106544  0.04083086  0.05660245  0.04554701  0.04138117\n",
      "   0.04228829  0.0426799   0.04650132  0.04694178]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.05319975  0.04747548  0.05253895  0.05744793  0.03412615  0.05682328\n",
      "   0.04716266 -0.44580338  0.04640596  0.05062322]\n",
      " [ 0.05167092 -0.43982096  0.04368713  0.06056199  0.04873319  0.04427593\n",
      "   0.0452465   0.04566551  0.04975426  0.05022553]\n",
      " [ 0.05168739 -0.43996116  0.04370105  0.0605813   0.04874872  0.04429004\n",
      "   0.04526093  0.04568007  0.04977012  0.05024154]\n",
      " [ 0.05308547  0.04737349  0.05242609  0.05732452  0.03405284  0.05670122\n",
      "   0.04706135 -0.44484573  0.04630627  0.05051448]]\n",
      "b1 relative error: 2.00e-02\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[  0.00000000e+00  -4.44089210e-11   2.11265350e+00   0.00000000e+00\n",
      "   0.00000000e+00  -2.33373854e-04  -6.66133815e-11  -3.20742161e+00\n",
      "   5.28565693e-03   0.00000000e+00   0.00000000e+00   2.63919051e-03\n",
      "   0.00000000e+00  -1.28569582e-03   7.94567900e-01   0.00000000e+00\n",
      "  -3.43154091e-01  -5.81271431e-04   0.00000000e+00   1.99840144e-10]\n",
      "b2 relative error: 4.44e-03\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00  -5.72243428e-03\n",
      "   0.00000000e+00   0.00000000e+00  -8.42554915e-06   0.00000000e+00\n",
      "   0.00000000e+00  -3.38678308e-03  -4.44089210e-11   0.00000000e+00\n",
      "   0.00000000e+00   2.89195468e-04  -7.99488697e-05   0.00000000e+00\n",
      "  -5.53070265e-03  -6.13875750e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.41317943e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.12805598e-04   5.44846790e-04\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "b3 relative error: 7.79e-11\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[ 0.1050082  -0.39259188  0.09635633  0.11816206  0.0829594   0.10123782\n",
      "  0.09253066 -0.40092675  0.09628374  0.10098042]\n",
      "beta1 relative error: 1.00e+00\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[ -2.13953689e-01   1.12454786e-01   7.33314261e-02  -1.32987799e-04\n",
      "   1.11677099e-01  -1.06697996e-01   2.22698340e-01  -7.34448877e-02\n",
      "   2.69928990e-01  -1.09329612e-04   3.38458681e-02   2.42163209e-01\n",
      "  -2.12459912e-01  -2.52402892e-01   1.56336919e-01   3.46389761e-04\n",
      "  -1.10107929e-01  -7.54777594e-02   2.08999285e-04  -1.89444105e-01]\n",
      "beta2 relative error: 1.00e+00\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[-0.00776922 -0.00901038 -0.03337288 -0.01329018 -0.01586736 -0.01097224\n",
      " -0.00111698 -0.01731685 -0.05411243 -0.03464685 -0.02274534  0.01298318\n",
      "  0.01563172  0.02584616 -0.01837337  0.02475835 -0.0391197  -0.03005042\n",
      "  0.00057262  0.00820659  0.01236508 -0.00341924  0.04804467 -0.02059758\n",
      " -0.01862988 -0.00555407  0.01450126  0.05828817 -0.02427482  0.01000644]\n",
      "gamma1 relative error: 1.36e-08\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[-0.21393773  0.11228801  0.06040838  0.          0.11165407 -0.10666725\n",
      "  0.22259775 -0.0557352   0.26959345  0.          0.03355581  0.24195957\n",
      " -0.21243373 -0.25227508  0.14822391  0.         -0.10601466 -0.07542741\n",
      "  0.         -0.18910772]\n",
      "gamma2 relative error: 4.07e-09\n",
      "[-0.21395369  0.11245479  0.07333143  0.          0.1116771  -0.106698\n",
      "  0.22269834 -0.07344489  0.26992899  0.          0.03384587  0.24216321\n",
      " -0.21245991 -0.25240289  0.15633692  0.         -0.11010793 -0.07547776\n",
      "  0.         -0.18944411]\n",
      "[-0.0075952  -0.00897228  0.         -0.01315995 -0.00683416 -0.01070159\n",
      " -0.00111625 -0.01728605 -0.05401233 -0.0345209  -0.00535133  0.          0.\n",
      "  0.02582404 -0.01836499  0.         -0.03893773 -0.03001199  0.\n",
      "  0.00479588  0.0122456   0.          0.04802057  0.         -0.01739865\n",
      "  0.          0.01447452  0.05824393 -0.02426412  0.00996654]\n",
      "\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  6.84780572438\n",
      "W1 relative error: 6.78e-06\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[[-0.16232819 -0.10760997  0.00472525 -0.18826316 -0.15339795  0.02287331\n",
      "   0.10409628 -0.02780187 -0.03825784 -0.24044828  0.0084452  -0.07412239\n",
      "   0.17043648  0.08898395 -0.21716421 -0.03608505 -0.13311772 -0.10299103\n",
      "  -0.34979186 -0.06922538]\n",
      " [ 0.24790182 -0.10923787 -0.24832579  0.09666631 -0.09675873  0.15973892\n",
      "   0.19357675  0.26864055 -0.06808346 -0.06312719  0.1133114  -0.00313106\n",
      "   0.17830847  0.15120988  0.02620022 -0.00981685  0.12376191  0.28287555\n",
      "  -0.11206075  0.23990364]\n",
      " [ 0.11634893  0.14728169  0.12716853  0.0266489  -0.02669384 -0.1162868\n",
      "  -0.00367415  0.18125391  0.13611981  0.15262961 -0.06719458 -0.14045964\n",
      "   0.27609295 -0.13593142  0.1464311   0.17834127 -0.13697528 -0.01441742\n",
      "  -0.30022251  0.0160338 ]\n",
      " [-0.11165863 -0.09413023 -0.03519384 -0.06382433 -0.33681994 -0.13515373\n",
      "  -0.12958491  0.11000388 -0.25000153 -0.24867181  0.17809117 -0.19939028\n",
      "   0.1829477  -0.02414645 -0.10309684 -0.12282849  0.19768974  0.12102523\n",
      "  -0.12891209 -0.22854316]\n",
      " [-0.04809126 -0.10965279 -0.05667904  0.10846297 -0.24878708 -0.00147459\n",
      "   0.20216084 -0.21530016 -0.16064929 -0.00778064 -0.28117046 -0.40307799\n",
      "  -0.41284053 -0.13916499  0.0941207   0.1477809  -0.12162806 -0.13349817\n",
      "  -0.17093996 -0.00389254]\n",
      " [ 0.18463373  0.3317305  -0.1882929   0.03300661 -0.07235042 -0.17312545\n",
      "   0.06266229 -0.35636941 -0.11835306  0.03234067 -0.23756511  0.07233665\n",
      "   0.14386622  0.08873385 -0.03319985 -0.04226759 -0.2631242   0.07585846\n",
      "  -0.16849262 -0.05529439]\n",
      " [-0.0419417   0.08869953  0.13046281  0.22145801 -0.02740634 -0.08680816\n",
      "  -0.06798714 -0.17371737  0.06207942 -0.25246254 -0.16978037 -0.10856927\n",
      "  -0.03425966 -0.10203657  0.04991886  0.01140701  0.07289051 -0.098326\n",
      "   0.17237619  0.04353086]\n",
      " [-0.2100354   0.21937719  0.21750294 -0.10719786  0.27246563  0.11865862\n",
      "   0.25469752  0.03074242 -0.16322578 -0.0843514  -0.13596009  0.14925072\n",
      "  -0.13746226  0.07291477  0.00142958  0.27835603 -0.15504447  0.09057296\n",
      "   0.01840518 -0.07423921]\n",
      " [ 0.13130987  0.03651568  0.16242434 -0.11752357 -0.08901141  0.13310031\n",
      "   0.07347312 -0.20960084  0.30926478 -0.07069264 -0.09558721 -0.18020389\n",
      "  -0.09975793 -0.23814449 -0.08027589  0.01754386 -0.32365517  0.23186765\n",
      "  -0.08361963 -0.00058109]\n",
      " [ 0.23292279 -0.15294524  0.04937066  0.00418625 -0.01510829 -0.17068374\n",
      "   0.04166692  0.11297875 -0.032682   -0.15849845  0.00482722 -0.05935557\n",
      "   0.06870994  0.06119239  0.04396686  0.27230944  0.00834901  0.04872345\n",
      "  -0.02120042  0.19134795]\n",
      " [-0.46000881  0.03060925 -0.1487901  -0.23950688  0.05167663 -0.2403696\n",
      "   0.05262312 -0.24968174  0.05069768  0.01599334  0.13181268  0.01510425\n",
      "  -0.16862622  0.10196085 -0.09917119  0.1340371  -0.09010002  0.00699299\n",
      "   0.04401026 -0.00427942]\n",
      " [-0.16371254  0.20591228 -0.04468136  0.12109577 -0.11769634  0.15057384\n",
      "   0.11250316 -0.1495573  -0.22265116  0.00857824  0.06354319 -0.05935037\n",
      "  -0.08402745 -0.09431659 -0.16762979 -0.09630667 -0.02983969 -0.05544233\n",
      "   0.09949181  0.15206086]\n",
      " [ 0.09877869  0.39481072  0.01461993 -0.02619511  0.01996311  0.11211545\n",
      "   0.12240239  0.09155307 -0.02872633 -0.15499447  0.13664833 -0.06562924\n",
      "   0.16427134 -0.05900226  0.04164684  0.054322   -0.11101658 -0.1820731\n",
      "  -0.03820406 -0.02409655]\n",
      " [ 0.02190289 -0.15107302  0.10457793  0.02864091  0.20880545  0.06269872\n",
      "  -0.11769175  0.10066548  0.06325207  0.00367117  0.16931935 -0.03465841\n",
      "   0.35246824  0.00463773  0.19738205 -0.3639265   0.2266284  -0.07127633\n",
      "   0.04707135 -0.08673819]\n",
      " [-0.16816281 -0.41993978  0.14707826 -0.00362105  0.06907308 -0.39855441\n",
      "   0.05806866  0.09963194 -0.13067429  0.05879225 -0.03685204 -0.05574787\n",
      "   0.17447831  0.06176861 -0.05907846 -0.06749482  0.05714259 -0.11258225\n",
      "  -0.08481817  0.12681115]]\n",
      "W2 relative error: 6.46e-07\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[[ -4.27208834e-02   2.15349338e-01   9.33670509e-02  -1.48222637e-01\n",
      "   -2.79030044e-01   3.01304198e-02   5.56000811e-02   2.54343111e-03\n",
      "   -1.90758256e-01  -3.25748626e-02  -4.17669888e-02  -2.01832714e-01\n",
      "    7.05495213e-02   1.35130753e-01  -6.34226701e-02   1.43451047e-01\n",
      "    2.48722152e-01  -5.44203554e-02  -3.26182963e-01  -4.89190874e-03\n",
      "    1.17045444e-01   5.26823001e+00   1.06385462e-01   1.17113862e-01\n",
      "   -2.36622546e-01  -7.66571842e-03   2.24848477e-01   1.29471653e-01\n",
      "    8.21539646e-02  -1.06143740e-01]\n",
      " [ -1.48605869e-01   2.51729104e-02  -1.27799333e-01   2.39565793e-01\n",
      "    5.42136954e-02   1.81367214e-01   1.25105275e-01   1.72114576e-01\n",
      "    1.02395551e-01  -1.44211944e-01  -2.35219608e-01   1.91203936e-01\n",
      "   -1.20631036e-01   9.58732556e-03   7.44193167e-02   5.62820945e-03\n",
      "   -7.22032053e-02  -3.74225596e-01  -2.20039106e-01  -4.63586214e-02\n",
      "    4.40056120e-02   1.17285013e-01   1.62863751e-01  -2.39235837e-01\n",
      "   -1.68023323e-01  -3.01210065e-02  -1.81091687e-01  -2.15851695e-01\n",
      "    2.33739443e-02  -7.02836787e-02]\n",
      " [ -1.88609472e-02   1.49804868e-01  -8.71890116e-02  -8.14346707e-02\n",
      "   -8.63484911e-02   3.31707953e-01  -2.49217443e-02   2.95479069e-01\n",
      "   -6.50949873e-02   6.27309682e-02   3.78934571e-01  -3.01427624e-01\n",
      "    1.31180622e-02  -1.17432461e-01  -7.19991552e-02   2.60670929e-01\n",
      "   -1.20487394e-01   6.11661849e-02   1.24317711e-01   2.25060783e-02\n",
      "   -7.66416766e-02  -1.10218164e-02   1.18598292e-01  -8.82028468e-03\n",
      "    1.14781425e-01   3.33799244e-01   2.28688960e-01   3.03329251e-01\n",
      "   -4.55238567e-02   8.77663622e-02]\n",
      " [ -1.64162947e-01   1.64151693e-01  -1.90127991e-01   4.64562384e-02\n",
      "   -1.94159488e-04  -7.36348773e-03   1.32031921e-01   1.47518346e-01\n",
      "   -2.11037169e-01  -1.63581170e-01  -1.87603389e-01   1.26589496e-02\n",
      "    5.81975669e-02   9.50806586e-02  -1.80447237e-01   5.46051321e-02\n",
      "   -2.79649656e-02   2.74252580e-01   6.85121617e-02   3.84136449e-01\n",
      "   -8.98943029e-02   5.50127871e+00   3.68567287e-01  -8.20457182e-03\n",
      "    5.55662057e-02  -3.66685084e-02   8.42097663e-02   2.11728973e-01\n",
      "    2.16534115e-01  -3.02503008e-02]\n",
      " [ -2.74408927e-01   1.72149104e-01  -1.55692783e-01   1.47533452e-01\n",
      "   -2.35561091e-01  -1.21391505e-01  -9.92130716e-02  -3.04595649e-02\n",
      "   -3.53743685e-02   2.11394751e-01  -3.13157530e-01  -6.71925742e-03\n",
      "    2.09628850e-02   1.98485894e-02  -8.59941678e-02  -3.67354975e-02\n",
      "    7.20587585e-03   5.94062377e-02  -9.92696643e-02  -1.87501038e-01\n",
      "    1.21883722e-01   3.17201699e-02   9.42639157e-02  -6.82436389e-02\n",
      "    2.20680803e-03   1.47540217e-01  -3.24207232e-02   3.73503703e-01\n",
      "   -1.42054069e-01  -9.53083618e-02]\n",
      " [ -7.29224015e-02   1.02781353e-01  -3.02244598e-02  -3.86567718e-02\n",
      "    2.70241259e-02   9.89991491e-02   3.69961154e-02  -1.53656404e-01\n",
      "   -2.71964433e-01   7.24082421e-03  -3.36701419e-02   6.62917231e-02\n",
      "   -2.64426986e-02  -1.27141508e-01   1.49869881e-01  -5.43719028e-02\n",
      "    1.49556562e-01  -8.99662253e-02   2.17547628e-01  -5.64042722e-02\n",
      "    1.01564776e-01   1.00201703e-01   6.28488890e-02   1.88337420e-01\n",
      "    7.35368968e-02  -3.42782189e-03   1.07251318e-02   2.50537145e-01\n",
      "    1.15573839e-01  -2.02856640e-01]\n",
      " [  1.00509371e-01   1.61883324e-02   2.41591908e-01  -9.66602946e-02\n",
      "   -1.24774273e-01  -7.12048911e-02   5.43839038e-01   6.26555001e-02\n",
      "    5.76424662e-02   4.92232050e-02   9.15389225e-02   7.36720036e-03\n",
      "    2.27915464e-01  -1.59423120e-01   8.59548257e-02  -1.73608161e-02\n",
      "   -1.40423599e-02  -2.83610578e-02  -7.14103895e-02  -6.90871671e-02\n",
      "   -2.09320310e-01  -3.97556104e-01   4.07531199e-02  -1.01620439e-01\n",
      "    7.45401173e-02   4.45178173e-02   1.71221517e-01   2.48021199e-02\n",
      "   -1.14106062e-01   1.35049343e-01]\n",
      " [  2.95293196e-01  -1.93192299e-01  -2.74189818e-02   1.45851845e-01\n",
      "    2.17494136e-01  -1.10622551e-01   1.62241412e-01  -5.08565218e-02\n",
      "   -1.23884439e-01  -1.08891544e-01  -1.51718573e-01   2.38075833e-01\n",
      "   -2.69675446e-02  -5.84444280e-03   8.05394824e-02   9.50698737e-02\n",
      "    2.37674795e-01   1.44571415e-01   2.28353167e-02  -2.36659806e-01\n",
      "    2.11703804e-02  -2.12572467e-01   2.56963109e-01   2.31319475e-01\n",
      "    2.72074852e-02  -1.74144631e-01   2.24200714e-02   5.62042096e-02\n",
      "   -2.22315835e-01  -1.99452837e-01]\n",
      " [ -2.91202809e-01   1.63300123e-02  -8.80098700e-02  -2.30432243e-01\n",
      "    3.58261772e-01  -1.50513559e-01   8.91318564e-02  -1.42544661e-01\n",
      "   -3.48253906e-03  -8.72445979e-02  -3.47070124e-01  -5.64217447e-02\n",
      "    1.50680311e-01  -1.35258984e-01   2.42608934e-01  -1.00668184e-01\n",
      "   -2.02461386e-01  -9.63861108e-02  -1.47839689e-01  -1.28571879e-01\n",
      "   -4.30568941e-02   5.30780733e+00  -1.07895284e-01  -3.88413987e-02\n",
      "   -9.16329302e-02  -4.30373984e-02   1.91906725e-01  -1.46603561e-01\n",
      "   -8.18783330e-02  -6.22655964e-02]\n",
      " [  1.01789423e-01   1.14881686e-01   6.84440636e-02  -2.77425747e-02\n",
      "    9.88444315e-02  -4.15725291e-01  -7.48804938e-02   7.83398728e-02\n",
      "   -1.61025936e-01  -1.55627980e-01  -3.16311190e-01  -8.52286252e-02\n",
      "    1.17449307e-02  -1.21798762e-01   1.73981498e-01  -4.53593330e-02\n",
      "   -3.41777054e-02  -1.34655025e-01  -1.07560675e-01  -1.63199699e-01\n",
      "   -1.08342186e-01   5.58719534e+00  -3.89710552e-01   1.98630280e-01\n",
      "   -7.30851576e-02  -1.23169401e-01  -1.14268803e-01  -2.71628254e-01\n",
      "   -4.21748312e-02   2.07402704e-01]\n",
      " [  7.21572191e-02   5.94212259e-02   6.25241305e-02   3.53512921e-02\n",
      "    4.57752160e-02  -3.66057677e-01  -7.24521970e-02  -9.32647231e-03\n",
      "    1.65879343e-02   1.19575108e-01  -1.02248727e-01  -1.73068533e-02\n",
      "   -2.11627425e-01   4.21946749e-03   2.18052230e-01   8.86948693e-02\n",
      "   -6.55045728e-02   7.76382317e-02   7.29730790e-02  -4.57546308e-03\n",
      "    3.62163575e-01  -3.82031902e-02   1.84868106e-01  -1.88057137e-01\n",
      "   -2.47267028e-01   1.54027177e-01  -7.99362239e-03  -2.19708503e-01\n",
      "   -1.28253239e-01   1.75274615e-01]\n",
      " [ -4.83172491e-02  -1.76167125e-01  -3.30665500e-01   2.32955848e-01\n",
      "   -2.62429679e-01   2.93779241e-01  -1.58532748e-01  -2.62774528e-01\n",
      "   -5.01473514e-02   8.90128656e-02   1.34485377e-01  -1.57708115e-01\n",
      "    1.59467377e-01  -2.95069890e-01   1.25678036e-01   1.84512692e-02\n",
      "   -1.52683110e-01   1.36776621e-01   1.44459013e-02  -1.26947185e-01\n",
      "   -3.82782370e-02   2.50246768e-01   3.51680085e-02  -9.98401301e-02\n",
      "   -1.11472132e-01  -1.20052393e-02  -1.37690965e-01   6.03723311e-03\n",
      "   -1.88058590e-01   6.61699922e-02]\n",
      " [  4.10794307e-02  -1.23202125e-01  -7.08921926e-02  -6.52512506e-02\n",
      "    3.11993165e-01  -1.65530620e-01   6.26922350e-02  -9.29017807e-02\n",
      "    6.16728778e-02  -2.91847171e-02   8.18582820e-02  -2.18994485e-01\n",
      "    1.85879318e-01   1.86570877e-01   2.32687626e-01   4.24945527e-02\n",
      "    2.81866661e-01  -1.53290774e-01  -1.16475964e-01   1.66583041e-01\n",
      "   -9.82046018e-03   5.47800342e+00  -3.53183191e-01   1.00340407e-01\n",
      "    3.50096574e-04   1.71686644e-01  -1.03554379e-01  -1.78516768e-01\n",
      "   -2.31341685e-01  -5.62476592e-02]\n",
      " [ -1.88774625e-01   3.76668484e-02  -7.13222399e-02  -1.68460522e-01\n",
      "    2.03811417e-01   1.75425550e-01   8.32796953e-02  -5.80242513e-02\n",
      "    1.08818477e-01   1.37912732e-01   2.92274210e-02  -2.04333606e-01\n",
      "   -1.47288072e-01   8.80781074e-02   3.02723572e-01  -2.37725771e-01\n",
      "    3.33684279e-02  -5.78299903e-02   2.29707378e-01  -4.42861896e-02\n",
      "    8.07151465e-02   5.35546872e+00  -3.01184823e-02   2.35395516e-01\n",
      "    1.30142909e-01   1.84092627e-01   2.54832306e-01  -2.33082066e-01\n",
      "   -7.27476270e-03   7.55916471e-02]\n",
      " [ -1.87469431e-01  -1.94990238e-01   6.84481516e-02   1.97128309e-02\n",
      "   -1.01364597e-01   2.73197827e-02  -3.88694319e-02  -2.48042155e-02\n",
      "   -1.06532426e-01  -5.52464765e-02   1.25910398e-01   1.74640002e-01\n",
      "   -1.40944240e-01   5.89602446e-02  -7.32190280e-02   1.28551187e-01\n",
      "    2.09204798e-02  -2.35677084e-01   2.64796967e-01   2.11094216e-01\n",
      "   -8.85181482e-02   5.24874089e+00  -4.89269110e-02   3.00262147e-01\n",
      "   -1.53646493e-01   7.96965757e-02  -5.03021272e-02   4.48999876e-02\n",
      "    7.88520910e-02   7.65712239e-02]\n",
      " [ -7.63035501e-02  -2.76461927e-01   1.10482054e-01   1.79577311e-01\n",
      "    2.17139209e-01   8.76916495e-02  -4.12038958e-01  -2.31992753e-02\n",
      "   -5.61851594e-02   7.64008122e-02   2.29126138e-01  -1.25999769e-01\n",
      "   -9.88859076e-02   7.08499980e-02  -1.45232344e-01   4.92745765e-02\n",
      "    1.29679656e-01   1.90447360e-01  -1.23598221e-01  -5.16972699e-02\n",
      "   -1.17867303e-01   4.10289139e-02   8.68104882e-02  -2.55787533e-01\n",
      "   -1.75785378e-01  -1.28693893e-01  -9.53816573e-02   2.05425348e-01\n",
      "   -5.73191659e-02   9.66792393e-02]\n",
      " [  1.03533171e-01  -1.33053538e-02   4.29005152e-02  -4.05017642e-02\n",
      "    1.06062469e-01   3.54850610e-01  -2.14919366e-01  -8.66993496e-02\n",
      "    8.50940382e-02  -1.07215937e-01  -1.22197826e-01   8.73620416e-02\n",
      "    2.83147342e-01   1.94527524e-01   6.55276999e-02  -1.25235229e-01\n",
      "    5.36966403e-02   1.13143702e-01  -7.87773643e-02  -5.24826723e-02\n",
      "   -9.71455334e-03   5.54728240e+00   2.00549371e-01  -7.54163956e-02\n",
      "   -6.60949495e-02  -8.91153429e-02   1.14732133e-01   4.26809216e-02\n",
      "   -1.59264604e-01  -9.59173197e-02]\n",
      " [ -3.18623904e-01  -2.58062817e-01   9.22382349e-02   2.21569504e-01\n",
      "   -1.85525148e-02   3.07286118e-02   2.84286452e-03   1.70312822e-01\n",
      "    1.18925423e-01  -2.07059382e-02   1.10413969e-01  -7.20067835e-02\n",
      "    2.04621494e-01   1.14667857e-01   4.33138229e-01   3.23731336e-02\n",
      "   -1.23948467e-01  -8.40598284e-02   1.14851900e-02  -3.88079217e-02\n",
      "    2.48558144e-02   1.15931989e-01   3.22614265e-02   6.24468493e-02\n",
      "    2.08645300e-01  -4.79970277e-03  -5.03409235e-02  -1.07461593e-01\n",
      "   -1.91960557e-01   1.56646920e-01]\n",
      " [  6.88661294e-02   1.96995713e-01   5.20731111e-02  -2.99782137e-03\n",
      "    9.55669099e-02  -6.53268893e-02   5.77690868e-02   1.78203907e-01\n",
      "   -4.86675023e-02   2.90029556e-03   1.18227139e-01   6.41502292e-02\n",
      "   -7.94094134e-02   1.50682003e-01  -7.92313949e-02  -6.22512745e-02\n",
      "   -1.02130498e-01   1.54071261e-01   9.04760088e-02   3.49912512e-03\n",
      "    9.06515562e-02  -9.85357032e-03   8.59750704e-02   3.29452488e-03\n",
      "    2.57984210e-02  -1.87754466e-02  -1.61297891e-01  -1.57020839e-01\n",
      "    1.37363480e-01   3.00200843e-01]\n",
      " [  1.63946413e-01   2.45338226e-02  -1.61430410e-01  -2.24012848e-01\n",
      "    1.32460200e-01  -1.63070787e-01   1.33148233e-01  -2.29241713e-01\n",
      "    6.06369832e-02  -2.45289789e-01   1.25929572e-01  -1.91076569e-01\n",
      "    2.01673916e-02   2.61968733e-01   9.36293259e-02   1.26073269e-01\n",
      "   -1.12323308e-01  -8.50553019e-02  -2.49809632e-01   3.29251677e-02\n",
      "   -2.38295925e-02   5.56644534e+00  -7.65866912e-03  -2.70683299e-01\n",
      "    6.23495238e-03   3.54605700e-03   5.62589975e-05   8.17127861e-02\n",
      "   -7.25012653e-02   5.23744597e-02]]\n",
      "W3 relative error: 2.61e-08\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[[ -1.04533842e-01  -4.57279476e-01  -6.63868676e-02   9.18858047e-02\n",
      "   -2.80125595e-02   2.92938525e-01   1.20881419e-01  -4.64190911e-02\n",
      "    2.59475208e-02  -2.18635314e-01]\n",
      " [  2.95021788e-01  -3.27712942e-01   7.31834318e-03   1.98213264e-01\n",
      "   -1.55792423e-01   3.39813728e-02   1.06262063e-01   1.11711546e-01\n",
      "   -1.47194243e-01   8.99104669e-02]\n",
      " [  1.35766695e-01   1.18651803e-01   9.82532423e-02  -1.19477944e-01\n",
      "    1.02033007e-01   5.10543681e-02   1.10271138e-01  -3.75744417e-01\n",
      "    1.76238882e-01  -1.06909592e-01]\n",
      " [ -8.23758248e-03  -7.04349817e-01   1.76051795e-01  -7.18970936e-02\n",
      "    7.83978929e-02   3.60050835e-01   2.60551519e-01   9.83310007e-02\n",
      "   -7.95709498e-03   1.35367275e-01]\n",
      " [  1.98208730e-01  -6.76599008e-03   2.88711764e-01  -1.90121349e-01\n",
      "    1.09335834e-01  -1.12674522e-01   1.23020952e-01  -3.99326129e-01\n",
      "    1.26349511e-01   1.45911675e-01]\n",
      " [ -5.13076679e-02   1.03110545e-01   6.34529465e-03   1.04028029e-02\n",
      "   -8.44338033e-02  -3.06621194e-01   1.26083965e-01  -1.57520910e-01\n",
      "   -9.37964468e-02  -1.82192590e-01]\n",
      " [  2.22457790e-01  -7.45752741e-02   2.90084407e-02  -1.62567721e-01\n",
      "    1.33022945e-01   2.48049592e-02  -1.22581782e-01  -5.55429486e-01\n",
      "   -6.57132897e-02   2.76741414e-01]\n",
      " [  3.35651889e-01  -4.99220906e-01   4.81171479e-02  -1.08936219e-01\n",
      "    6.94053972e-02   1.94138456e-01   1.01614114e-01   1.29792617e-01\n",
      "    1.93031848e-02   3.02111491e-04]\n",
      " [ -3.48514539e-02  -3.70227460e-02   1.16421083e-01  -5.57932089e-02\n",
      "    7.34064878e-02  -5.47194760e-02  -7.43824311e-02  -3.23316117e-01\n",
      "   -1.78965630e-01   4.69121221e-02]\n",
      " [  1.64664406e-01  -6.04807520e-01  -4.72664277e-03   2.17165398e-01\n",
      "    1.29293323e-01   3.06630043e-01   2.44081420e-01   1.49321573e-01\n",
      "    9.88230029e-02   1.29971363e-03]\n",
      " [ -4.12079382e-01  -1.79475547e-01  -6.95252540e-03  -8.49422950e-02\n",
      "   -4.19061537e-02  -4.16273921e-02   3.33110979e-01  -7.88986781e-02\n",
      "   -1.22655104e-01  -7.57339882e-02]\n",
      " [ -2.34069475e-03  -3.36105205e-01   1.43957994e-01  -1.06171629e-01\n",
      "   -1.49354433e-01  -5.00393182e-02  -4.80931075e-02   1.88821182e-01\n",
      "   -2.97230751e-01   2.41993582e-01]\n",
      " [  1.42439297e-01   1.42392583e-01   8.72669221e-02   1.69406291e-01\n",
      "   -1.96817465e-03   8.44799122e-02   7.17747270e-02  -2.92821705e-01\n",
      "    1.89971123e-01   1.29086032e-01]\n",
      " [ -3.67759971e-02   3.13777679e-01   1.67618810e-01  -1.30192674e-01\n",
      "    2.92808948e-01   1.99868457e-01   2.21694039e-01  -2.03747934e-01\n",
      "   -9.82934288e-02   1.94359874e-02]\n",
      " [ -1.69513775e-01   3.12243770e-01   2.70850496e-01   1.46372434e-01\n",
      "    2.10155416e-01   2.29906289e-02  -2.29352958e-01  -4.51200676e-01\n",
      "   -1.61547550e-01   1.33861255e-01]\n",
      " [  2.96184558e-02  -5.01463398e-01   3.91151985e-02   5.05018444e-02\n",
      "    3.00158858e-01  -9.42268703e-02   9.68608004e-02  -1.91278682e-01\n",
      "    2.35386143e-01  -2.20560285e-02]\n",
      " [ -2.48437308e-02  -2.02158833e-01   1.97455481e-01   8.66744938e-02\n",
      "   -1.17303749e-01   8.17641076e-02  -7.77479752e-02  -7.13124326e-01\n",
      "    1.56100439e-01   1.41251155e-01]\n",
      " [  1.17217632e-01  -9.06526858e-01  -1.41544383e-01  -5.83355726e-02\n",
      "    1.87707585e-01   1.88786225e-01  -1.34222039e-02  -1.86011973e-02\n",
      "    7.18194952e-03   7.36167880e-02]\n",
      " [ -5.62794625e-02  -3.32649051e-01   2.91953095e-01  -1.58322943e-01\n",
      "    1.95866631e-01   2.15957494e-01   1.84378595e-01  -3.19741861e-02\n",
      "    6.29725856e-02   3.58893363e-01]\n",
      " [  1.35529747e-01  -1.58124797e-01   3.27768627e-02   4.71273371e-02\n",
      "   -5.82677104e-02   6.97278112e-02  -1.67385080e-02  -6.95933918e-01\n",
      "    3.84657519e-02   4.92692154e-01]\n",
      " [  3.14715968e-02  -6.30146702e-01   8.18426212e-02   1.34382179e-01\n",
      "   -2.78284285e-02   3.36131122e-01   1.64528887e-01   2.63538081e-01\n",
      "    1.06934897e-01  -1.97843145e-01]\n",
      " [ -4.79482790e-02  -3.43200761e-01   9.43833627e-02  -5.33953968e-02\n",
      "    1.18004901e-01  -7.03825992e-02   1.53064271e-01  -2.33345310e-01\n",
      "    5.80483439e-04   2.57165957e-01]\n",
      " [  3.59150456e-01  -5.40578613e-01  -1.58007393e-01  -1.09668073e-01\n",
      "    2.72907221e-02   2.32593981e-01  -9.33277414e-02   3.02568127e-01\n",
      "   -6.28441388e-03   2.07013316e-01]\n",
      " [  9.23749261e-02   3.68966691e-02   1.89056667e-01   3.49758915e-01\n",
      "    3.94693717e-02  -4.37003636e-02  -1.57861426e-02  -3.92536534e-01\n",
      "    2.13859742e-01   2.16972491e-01]\n",
      " [  5.21718406e-03   2.08275746e-02   4.72190751e-02   2.76583617e-02\n",
      "    3.80390012e-01  -4.03110700e-04  -2.27976743e-01   1.77166761e-01\n",
      "   -6.45680453e-02  -7.11413290e-02]\n",
      " [ -3.07532698e-01  -4.03776961e-01   2.55758783e-01  -6.61203464e-02\n",
      "    8.65151646e-02   1.13381229e-01  -2.84462508e-02   1.02986874e-01\n",
      "   -1.82178019e-01   3.35737516e-01]\n",
      " [ -6.50935036e-02  -1.30927261e-02  -6.70653328e-02   2.36772141e-01\n",
      "   -1.14940015e-01   2.84023738e-02  -1.31453582e-01  -6.93164640e-01\n",
      "    2.00346840e-01   5.39682810e-02]\n",
      " [  2.47974716e-02   6.21791734e-02   4.49165432e-02  -2.31450734e-01\n",
      "    1.51636902e-01  -1.74369423e-01   1.54071069e-01  -3.10205422e-01\n",
      "   -1.26192450e-01  -3.97866858e-02]\n",
      " [  8.25872177e-02  -1.25423859e-02  -2.94488571e-02  -9.70847539e-02\n",
      "    9.01541038e-02  -5.21198411e-02   1.12272481e-01   9.77639278e-02\n",
      "    1.17971870e-01   1.56282705e-01]\n",
      " [ -1.29083679e-01  -5.26215826e-01   2.43398271e-01  -9.18965180e-02\n",
      "    2.69526761e-01   9.02176681e-02  -8.71344508e-04   1.20153768e-01\n",
      "   -9.48716723e-02   1.46568627e-01]]\n",
      "b1 relative error: 1.34e-06\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[ -9.84406725e-03   0.00000000e+00   5.64064351e-06   3.72299550e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -1.95405554e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -3.37304851e-05   0.00000000e+00]\n",
      "b2 relative error: 1.84e-06\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[ -2.93253111e-03   0.00000000e+00  -3.94242003e-02   1.60148337e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.56784194e-04\n",
      "   0.00000000e+00   1.82722140e-03   0.00000000e+00  -2.98273379e-02\n",
      "  -1.49670498e-04   0.00000000e+00   0.00000000e+00   1.07631948e-04\n",
      "   1.25430516e-02   7.22951121e-04  -8.98883190e-04   7.91369903e-04\n",
      "   2.98073477e-04   5.44130525e+00   6.11822593e-05   1.33594469e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "b3 relative error: 2.87e-10\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[ 0.09863822 -0.41495543  0.11123797  0.08367037  0.10340362  0.11589067\n",
      "  0.09642834 -0.39926562  0.08810353  0.11684833]\n",
      "beta1 relative error: 1.00e+00\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[-0.28646533  0.10200563  0.0063703   0.1621107   0.00633771  0.08688156\n",
      " -0.34633816 -0.0022223  -0.14316667  0.25822915 -0.00463248  0.2199702\n",
      "  0.06857066 -0.13547178 -0.33184687  0.03625929  0.18492008  0.10066047\n",
      " -0.00476767  0.22099368]\n",
      "beta2 relative error: 1.00e+00\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[-0.00542241 -0.01856591 -0.0041245   0.04809028 -0.00263866 -0.00717168\n",
      "  0.01379068  0.0120046   0.02119218  0.03552078  0.00967464 -0.02466842\n",
      " -0.0143984  -0.02849753  0.00229069  0.00331312  0.03566316  0.06462324\n",
      " -0.00713765  0.03890965  0.03279403  0.03447094  0.01816155  0.00188165\n",
      " -0.01082732 -0.0089633   0.0290388  -0.02799289  0.0009047   0.01265463]\n",
      "gamma1 relative error: 8.08e-09\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[-0.2859471   0.          0.00636929  0.16106777  0.00633054  0.          0.\n",
      " -0.00202034 -0.14185082  0.25813665 -0.00461324  0.          0.06856117\n",
      " -0.13533444 -0.33166629  0.          0.18483909  0.         -0.00476467\n",
      "  0.22086883]\n",
      "gamma2 relative error: 7.39e-09\n",
      "[-0.28646533  0.          0.0063703   0.1621107   0.00633771  0.          0.\n",
      " -0.0022223  -0.14316667  0.25822915 -0.00463248  0.          0.06857066\n",
      " -0.13547178 -0.33184687  0.          0.18492008  0.         -0.00476767\n",
      "  0.22099368]\n",
      "[-0.00536051 -0.01847579 -0.00379357  0.04769356 -0.00263786  0.\n",
      "  0.01368159  0.01198877  0.          0.03543667  0.         -0.02418493\n",
      " -0.01438667 -0.02848648  0.00228986  0.00330735  0.035358    0.06456794\n",
      " -0.00710692  0.03886005  0.0327696   0.00114529  0.01815457  0.00188046\n",
      "  0.         -0.00894632  0.02901983 -0.02723366  0.          0.01258976]\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print 'Running check with reg = ', reg\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            use_batchnorm=True)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print 'Initial loss: ', loss\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))\n",
    "  if reg == 0: print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm for deep networks\n",
    "Run the following to train a six-layer network on a subset of 1000 training examples both with and without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "bn_solver.train()\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to visualize the results from two networks trained above. You should find that using batch normalization helps the network to converge much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o', label='baseline')\n",
    "plt.plot(bn_solver.loss_history, 'o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.train_acc_history, '-o', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(solver.val_acc_history, '-o', label='baseline')\n",
    "plt.plot(bn_solver.val_acc_history, '-o', label='batchnorm')\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization and initialization\n",
    "We will now run a small experiment to study the interaction of batch normalization and weight initialization.\n",
    "\n",
    "The first cell will train 8-layer networks both with and without batch normalization using different scales for weight initialization. The second layer will plot training accuracy, validation set accuracy, and training loss as a function of the weight initialization scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers = {}\n",
    "solvers = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print 'Running weight scale %d / %d' % (i + 1, len(weight_scales))\n",
    "  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=True)\n",
    "  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers[weight_scale] = bn_solver\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results of weight scale experiment\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:\n",
    "Describe the results of this experiment, and try to give a reason why the experiment gave the results that it did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
